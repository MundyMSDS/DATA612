---
title: "DATA 612 Final Project"
author: "Jim Mundy"
output:
  html_document:
    css: ./lab.css
    highlight: pygments
    theme: cerulean
    toc: false
    toc_float: false
  pdf_document: default
---



```{r message=FALSE, warning=FALSE, echo=FALSE}
library(recommenderlab)
library(data.table)
library(tidyverse)
library(broom)
library(readr)
library(DT)
library(knitr)
library(grid)
library(gridExtra)
library(corrplot)
library(qgraph)
library(methods)
library(Matrix)
library(skimr)
library(ggthemes)
library(scales)
library(shiny)
library(shinydashboard)
library(data.table)
library(shinyjs)
devtools::install_github("stefanwilhelm/ShinyRatingInput")
```


<hr class="my-4">

# {.tabset .tabset-fade}

 </br>

## Final Project

<div class="jumbotron">
  </br>
  <h3 class="display-3">Good Reads Book Recommender</h3>
  <p class="lead">Building a Production Ready Book Recommendation Engine</p>
  <hr class="my-4">
  <p>Author: Jim Mundy</p>
  <p>Date: July 2020 </p>
  </br>
  <h3 class="display-3">Game Plan</h3>
  </br>
  <p style="font-size:17px;">I have selected the Good Reads data set for my final project. The data set has approximately 1 million ratings and 10 thousand items. Per the Final Project requirements, I seek to produce a production-quality system that provides quality book recommendations.  The steps I will follow to achieve these objectives are set forth below:</p>    
 </br>
<ul>
  <li style="font-size:17px;">Load Data and Perform EDA</li> 
  <li style="font-size:17px;">Execute Required Preprocessing Tasks That Result From EDA</li>
  <li style="font-size:17px;">Build a Book Recommender Engine That Produces Quality Recommendations</li>
  <li style="font-size:17px;">Optimize the Engine For Production</li>
  <li style="font-size:17px;">Deploy the Recommender Engine Using Shiny</li>
</ul>
  </br>
  </br>
  </br>
  </br>
</div>


## Load Data 

<div class="jumbotron">
  <h3 class="display-3">Loading The DATA</h3>
  <hr class="my-4">
  <p style="font-size:17px;">We use the readr package and read_csv function to import the data.  The data set is comprised of three four csv files:</p>
  
 <ul>
  <li style="font-size:17px;">books - book meta data including title, author, isbn number, etc. </li> 
  <li style="font-size:17px;">rating - includes book_id, user_id and rating.</li>
  <li style="font-size:17px;">book_tags - join file comprised of book_id, tag_id and count.</li>
  <li style="font-size:17px;">tags - look-up table for tag names comprised of tag_id and tag_name.</li>
</ul>


</div>
<br/>

```{r message=FALSE, warning=FALSE}
books <- read_csv('books.csv')
ratings <- read_csv('ratings_cleaned.csv')
book_tags <- read_csv('book_tags.csv')
tags <- read_csv('tags.csv')
```




<p style="font-size:17px;">After loading the data, we take a quick glimpse to make sure everything loaded correctly.  Overall the data looks good.</p>

### Books

```{r}
glimpse(books)

```
### Ratings

```{r}

glimpse(ratings)

```

## EDA & Preprocessing

<div class="jumbotron">
  <h3 class="display-3">EDA & Preprocessing</h3>
  <hr class="my-4">
  <p style="font-size:17px;">To develop a better understanding of the data we use the skim function on the books and ratings data. Next we use ggplot to perform some additional EDA and to evaluate what, if any, preprocessing is required.  Here are some observation: </p>
  
 <ul>
  <li style="font-size:17px;">The fields with missing data include: ISBN, original tile, language code, original publication date.</li> 
  <li style="font-size:17px;">The missing data shouldn't impact my recommender engine performance</li>
  <li style="font-size:17px;">The ratings table is 100% complete with ratings from 1 to 5.</li>
  <li style="font-size:17px;">The vast majority of ratings appear to fall between 3 and 5</li>
  <li style="font-size:17px;">We will need to remove duplicate data.</li>
</ul>


</div>
<br/>

### Books

```{r echo=FALSE}
skim(books)
```

### Ratings

```{r echo=FALSE}
skim(ratings)

```


### ggplot2 EDA

<p style="font-size:17px;">The following chart facilitate a deeper understanding of the data set.  </p>
</br>

### Count of Ratings

<p style="font-size:17px;">There are relatively few 1 and 2 ratings.  Four was the most popular rating.</p>

```{r echo = FALSE}

ratings %>% 
  ggplot(aes(x = rating, fill = factor(rating))) +
  geom_bar() +
  guides(fill = FALSE) +
  scale_y_continuous(labels = comma) +
  ggtitle("Count of Ratings") +
  theme_fivethirtyeight() 
  

```

### Number of ratings per user

<p style="font-size:17px;">The ratings per user chart has a long-tail, with the many user providing a handful (1 to 5) ratings and far fewer users providing up to 200 ratings.</p>

```{r echo = FALSE}
ratings %>% 
  group_by(user_id) %>% 
  summarize(number_of_ratings_per_user = n()) %>% 
  ggplot(aes(number_of_ratings_per_user)) + 
  geom_bar() + 
  scale_y_continuous(labels = comma) +
  geom_bar(fill = "blue", color = "blue") +
  ggtitle("Ratings Per User") +
  theme_fivethirtyeight()
  
```



### Distribution of Genres

<p style="font-size:17px;">The data set includes more than 25 different genres.  The most popular genres include fantasy, romance and mystery.  Sports, manga and cookbooks were the least represented genres.</p>

```{r echo=FALSE}
genres <- str_to_lower(c("Art", "Biography", "Business", "Chick Lit", "Children's", "Christian", "Classics", "Comics", "Contemporary", "Cookbooks", "Crime", "Ebooks", "Fantasy", "Fiction", "Gay and Lesbian", "Graphic Novels", "Historical Fiction", "History", "Horror", "Humor and Comedy", "Manga", "Memoir", "Music", "Mystery", "Nonfiction", "Paranormal", "Philosophy", "Poetry", "Psychology", "Religion", "Romance", "Science", "Science Fiction", "Self Help", "Suspense", "Spirituality", "Sports", "Thriller", "Travel", "Young Adult"))

exclude_genres <- c("fiction", "nonfiction", "ebooks", "contemporary")
genres <- setdiff(genres, exclude_genres)

available_genres <- genres[str_to_lower(genres) %in% tags$tag_name]
available_tags <- tags$tag_id[match(available_genres, tags$tag_name)]

tmp <- book_tags %>% 
  filter(tag_id %in% available_tags) %>% 
  group_by(tag_id) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(sumN = sum(n), percentage = n / sumN) %>%
  arrange(-percentage) %>%
  left_join(tags, by = "tag_id")

tmp %>% 
  ggplot(aes(reorder(tag_name, percentage), percentage, fill = percentage)) +
  geom_bar(stat = "identity") + 
  coord_flip() + 
  scale_fill_distiller() + 
  labs(y = 'Percentage', x = 'Genre') +
  ggtitle("Ratings Per User") +
  theme_fivethirtyeight()

```

<br>

### Top 10 rated books

<p style="font-size:17px;">Finally, the table below sets forth the Top 10 books by average rating. Three different Harry Potter books made the Top 10.</p>
</br>

```{r echo = FALSE}
books %>% 
  mutate(image = paste0('<img src="', small_image_url, '"></img>')) %>% 
  arrange(-average_rating) %>% 
  top_n(10,wt = average_rating) %>% 
  select(image, title, ratings_count, average_rating) %>% 
  datatable(class = "nowrap hover row-border", escape = FALSE, options = list(dom = 't',scrollX = TRUE, autoWidth = TRUE))
```

<br><br>



### Preprocessing

<p style="font-size:17px;">The EDA leads me to implement one preprocessing task - Removing duplicate ratings pairs, if any. </p>

 
```{r}

ratings <- ratings %>% 
  group_by(user_id, book_id) %>% 
  mutate(n=n()) %>% 
  filter(n==1) %>% 
  ungroup()



```




## Build Engine

<div class="jumbotron">
  <h3 class="display-3">Build and Evaluate Recommendation Engine</h3>
  <hr class="my-4">
  <p style="font-size:17px;">We will utilize RecommenderLabs to build and evaluate our recommendation engine. We will build a User Collaborative Model as our base line model.  We will also consider alternative algorithms in an effort to find the algorithm that produces the best possible recommendations.  The key steps to our analysis include: </p>
  
 <ul>
  <li style="font-size:17px;">Create ratings matrix</li> 
  <li style="font-size:17px;">Build UBCF Model</li>
  <li style="font-size:17px;">Create Prediction of UBCF Model</li>
  <li style="font-size:17px;">Evaluate and Tune UBCF Model</li>
  <li style="font-size:17px;">Evaluate Alternative Algorithms</li>
  <li style="font-size:17px;">Pick the Best Algorithm</li>
</ul>
</div>
<br/>

```{r, echo=FALSE, eval=FALSE}
matrix(data = c(NA, NA, 4, NA, NA, 2, 1, NA, NA, NA, NA, NA, 3, NA, 3), nrow = 3, ncol = 5, byrow = TRUE, dimnames = list(user_id = 1:3, book_id = 1:5))
```

### Create Rating <Matrix

<p style="font-size:17px;">First we use dplyr to restructure the ratings data frame. Then we convert the data frame to a matrix. </p>

```{r}
dimension_names <- list(user_id = sort(unique(ratings$user_id)), book_id = sort(unique(ratings$book_id)))

ratingmat <- spread(select(ratings, book_id, user_id, rating), book_id, rating) %>% 
  select(-user_id)

ratingmat <- as.matrix(ratingmat)
dimnames(ratingmat) <- dimension_names
#ratingmat[1:5, 1:5]

```


<p style="font-size:17px;">Next we replace NAs with zeros and use Recommederlabs (data=sparse_ratings) to create a sparse ratings matrix.  The sparse matrix format reduces the memory footprint of the model. We have just under 1 million (960,595) ratings in our ratings matrix.</p>

```{r ,results='hide', message=FALSE, warning=FALSE}
ratingmat0 <- ratingmat
ratingmat0[is.na(ratingmat0)] <- 0
sparse_ratings <- as(ratingmat0, "sparseMatrix")
rm(ratingmat0)
gc()
```


```{r}
real_ratings <- new("realRatingMatrix", data = sparse_ratings)
real_ratings
```


### Build Model

<p style="font-size:17px;">Now we build the model using the Recommender function, the UBCF algorithm and the Pearson method to calculate similarity.</p>

```{r cache=TRUE}
model <- Recommender(real_ratings, method = "UBCF", param = list(method = "pearson", nn = 4))
```


### Create Predictions

<p style="font-size:17px;">Now we designate a user (user 4763) and use the predict function to make a prediction.</p>

```{r cache=TRUE}
current_user <- "4763"

prediction <- predict(model, real_ratings[current_user, ], type = "ratings")
```


<p style="font-size:17px;">Here are the prediction results for user 4763</p>

```{r cache=TRUE, echo=FALSE}
as(prediction, 'data.frame') %>% 
  arrange(-rating) %>% .[1:5,] %>% 
  mutate(book_id = as.numeric(as.character(item))) %>% 
  left_join(select(books, authors, title, book_id), by = "book_id") %>% 
  select(-item) %>% 
  datatable(class = "nowrap hover row-border", escape = FALSE, options = list(dom = 't',scrollX = TRUE, autoWidth = TRUE))  
```
<br>   


### Tune the model

<p style="font-size:17px;">We'll utilize 10-fold cross validation and different values of nn to tune our recommendation model. We'll set k to 10 and given to -1.  The given of negative 1 means all but one rating is used to make the predictions. Performance is then evaluated for that 1 for each user. </p>

```{r}
scheme <- evaluationScheme(real_ratings[1:500,], method = "cross-validation", k = 10, given = -1, goodRating = 4)
```

<p style="font-size:17px;">Now we tune our model with values of nn set to 20, 30, 40, 50.  Tuning results are plotted below. We see that nn = 40 produces the best results, but its not significantly better than nn = 20, 30 or 50.</p>

```{r nn-comparison, warning=FALSE, message=FALSE, cache=TRUE}
algorithms <- list("random" = list(name = "RANDOM", param = NULL),
                   "UBCF_20" = list(name = "UBCF", param = list(nn = 20)),
                   "UBCF_30" = list(name = "UBCF", param = list(nn = 30)),
                   "UBCF_40" = list(name = "UBCF", param = list(nn = 40)),                   
                   "UBCF_50" = list(name = "UBCF", param = list(nn = 50))
                   )
# evaluate the alogrithms with the given scheme            
results <- evaluate(scheme, algorithms, type = "ratings")

# restructure results output
tmp <- lapply(results, function(x) slot(x, "results"))
res <- tmp %>% 
  lapply(function(x) unlist(lapply(x, function(x) unlist(x@cm[ ,"RMSE"])))) %>% 
  as.data.frame() %>% 
  gather(key = "Algorithm", value = "RMSE")
```


<p style="font-size:17px;">The UBCF model had similar values across the different values for nn.  The best performance was achieved when nn was set to 40.</p>

```{r}
res %>% 
  ggplot(aes(Algorithm, RMSE, fill = Algorithm)) +
  geom_bar(stat = "summary") + geom_errorbar(stat = "summary", width = 0.3, size = 0.8) +
  coord_cartesian(ylim = c(0.6, 1.3)) + guides(fill = FALSE) +
  ggtitle("Tuning Results") +
  theme_fivethirtyeight()
  
```


### Compare UBCF to SVD

<p style="font-size:17px;">Given the SVD model's ability to deploy at scale and our desire to deploy a model in Shiny, we'll compare our baseline UBCF model to an SVD below. All things, being equal SVD would likely be a better algorithm for deploying a production system because of its ability to make prediction quickly.</p>

```{r fig.width=5, cache=TRUE, message=FALSE, warning=FALSE}
scheme <- evaluationScheme(real_ratings[1:500,], method = "cross-validation", k = 10, given = -1, goodRating = 5)

algorithms <- list(
                   "UBCF" = list(name = "UBCF"),
                   "SVD" = list(name = "SVD")
                   )
                   
results <- evaluate(scheme, algorithms, type = "ratings", progress = FALSE)

# restructure results output
tmp <- lapply(results, function(x) slot(x, "results"))
res <- tmp %>% 
  lapply(function(x) unlist(lapply(x, function(x) unlist(x@cm[ ,"RMSE"])))) %>% 
  as.data.frame() %>% 
  gather(key = "Algorithm", value = "RMSE")
```

<p style="font-size:17px;">The UBCF outperforms SVD, so given all things were not equal we will deploy our engine utilizing our baseline UBCF model. </p>

```{r message=FALSE, warning=FALSE}
res %>% 
  mutate(Algorithm=factor(Algorithm, levels = c("UBCF", "SVD"))) %>%
  ggplot(aes(Algorithm, RMSE, fill = Algorithm)) + geom_bar(stat = "summary") + 
  geom_errorbar(stat = "summary", width = 0.3, size = 0.8) + coord_cartesian(ylim = c(0.6, 1.3)) + 
  guides(fill = FALSE) +
  ggtitle("UBCF vs SVD") +
  theme_fivethirtyeight()
```




## Optimize

<div class="jumbotron">
  <h3 class="display-3">Optimize the engine for production</h3>
  <hr class="my-4">
  <p style="font-size:17px;">We have picked our baseline UBCF model to deploy.  The problem with that choice is that RecommenderLabs is more of an educational/analytical tool and not ideally suited for production systems owing to the fact that its rather slow producing predictions. Normally, one might turn to Spark or Spark and Sparklyr deploy strategy.  The Spark solution is well suited for production system because it holds everything in memory and produce predictions very efficiently and fast.  </p>
  

<p style="font-size:17px;">While I was researching final project alternatives, however, I came across a blog post that demonstrates an efficient R implementation of User Based Collaborative Filtering.</p>

[Blog Post - Improved UCBF](https://blog.smartcat.io/2017/improved-r-implementation-of-collaborative-filtering/)

<p style="font-size:17px;">The blog post sets forth the following steps to implement an improved UBCF model:</p>

* Take a ratings matrix.
* Normalize Matrix (optional)
* Calculate similarities between users.
* Use the k nearest neighbor approach (keep only k most similar users)
* Calculate predictions and denormalize them in case normalization was performed

<p style="font-size:17px;">Two main optimization steps are summarized below:</p>

<p style="font-size:17px;">The calculation of similarities is optimized for large sparse matrices. </p>

<p style="font-size:17px;">k nearest neighbors on similarity matrices were not calculated in a loop, but rather using an optimized implementation. All the values from the similarity matrix were grouped by column. In each group (column), the k-th highest value were found and only the k highest values per column in the similarity matrix were kept and utilized for predictions.</p>

<p style="font-size:17px;">You can see some similarities to matrix decomposition/reduction utilized by SVD models.</p>

<p style="font-size:17px;">Two function did all the heavy lifting in this approach: cf_algorithm.R (collaborative filtering) and
similarity_measures.R.  The link below is to my github repository where these functions can be reviewed.</p> 


</div>
<br/>


## Live Engine


### Click the Link Below To Access The Live Book Recommender Engine


[Shiny App - Book Recommendation Engine](https://mundymsds.shinyapps.io/JMundy_Data612FinalProject_Shiny/) 


## Resources

<p style="font-size:17px;">List of resouces leveraged in the project:</p>

* Kaggle Good Books Data Set - Book Recommender
* Improved R implementation of collaborative filtering = https://blog.smartcat.io/2017/improved-r-implementation-of-collaborative-filtering/
* Market Basket Analysis with recommenderlab - https://towardsdatascience.com/market-basket-analysis-with-recommenderlab-5e8bdc0de236
* For Recommenderlab Package Vignette see: https://cran.r-project.org/web/packages/recommenderlab/vignettes/recommenderlab.pdf
* Smart Cats Blog Post - : //blog.smartcat.io/2017/improved-r-implementation-of-collaborative-filtering/
* RStudio - Shiny Tutorial
* Dean Atalli - Shiny javascript resoures
* ShinyRatingInput (javascript star ratings) - Stefan Wilhelm.
* shinyapps.io - deployed shiny app here.
